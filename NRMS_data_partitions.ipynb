{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Here we import all libraries needed**\n",
        "\n"
      ],
      "metadata": {
        "id": "laucAiJGh5tg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGdGdZ1MOv5n"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here we connect our program to google disc and load input data from it**"
      ],
      "metadata": {
        "id": "6dT-O2fFiJDz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOxtE_LBPCv8",
        "outputId": "cb65446f-0fa0-4b7f-d080-52e283a1b82e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive if using datasets stored there\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Dx4yPBjPCs0"
      },
      "outputs": [],
      "source": [
        "# Set file paths (modify these paths based on where your files are stored)\n",
        "train_path = '/content/drive/MyDrive/deep_learning_project/data/train'  # Adjust to the correct path\n",
        "valid_path = '/content/drive/MyDrive/deep_learning_project/data/validation'  # Adjust to the correct path\n",
        "test_path = '/content/drive/MyDrive/deep_learning_project/data/test'  # Adjust to the correct path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5TBHs8OPCqM"
      },
      "outputs": [],
      "source": [
        "# Load the train dataset\n",
        "df_train_history = pd.read_parquet(os.path.join(train_path, 'history.parquet'))\n",
        "df_train_behaviors = pd.read_parquet(os.path.join(train_path, 'behaviors.parquet'))\n",
        "df_train_articles = pd.read_parquet(os.path.join(train_path, 'articles.parquet'))\n",
        "\n",
        "# Load the validation dataset\n",
        "df_valid_history = pd.read_parquet(os.path.join(valid_path, 'history.parquet'))\n",
        "df_valid_behaviors = pd.read_parquet(os.path.join(valid_path, 'behaviors.parquet'))\n",
        "df_valid_articles = pd.read_parquet(os.path.join(valid_path, 'articles.parquet'))\n",
        "\n",
        "# Load the test dataset\n",
        "df_test_history = pd.read_parquet(os.path.join(test_path, 'history.parquet'))\n",
        "df_test_behaviors = pd.read_parquet(os.path.join(test_path, 'behaviors.parquet'))\n",
        "df_test_articles = pd.read_parquet(os.path.join(test_path, 'articles.parquet'))\n",
        "\n",
        "# Delete the article_ids_clicked in test set (so far we use validation set as a test set in true test set there is no such a column so we have to prepare the model for it)\n",
        "df_test_behaviors.drop('article_ids_clicked', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ1TehMbPCnU"
      },
      "outputs": [],
      "source": [
        "# Ensure consistent data types for merging in train dataset\n",
        "df_train_behaviors['article_id'] = df_train_behaviors['article_id'].fillna('-1').astype(str)\n",
        "df_train_articles['article_id'] = df_train_articles['article_id'].astype(str)\n",
        "\n",
        "# Ensure consistent data types for merging in validation dataset\n",
        "df_valid_behaviors['article_id'] = df_valid_behaviors['article_id'].fillna('-1').astype(str)\n",
        "df_valid_articles['article_id'] = df_valid_articles['article_id'].astype(str)\n",
        "\n",
        "# Ensure consistent data types for merging in test dataset\n",
        "df_test_behaviors['article_id'] = df_test_behaviors['article_id'].fillna('-1').astype(str)\n",
        "df_test_articles['article_id'] = df_test_articles['article_id'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_user_ids = df_train_history['user_id'].unique()\n",
        "user_id_to_index = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}"
      ],
      "metadata": {
        "id": "_IoYMLEPUBkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/deep_learning_project/data/user_id_to_index.pkl', 'wb') as f:\n",
        "    pickle.dump(user_id_to_index, f)"
      ],
      "metadata": {
        "id": "vzcV-vxiUDrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHDtILv8PCaI"
      },
      "outputs": [],
      "source": [
        "# EMBEDDINGS OF ARTICLES\n",
        "\n",
        "# Import the embedding fle provided by the competition organizers\n",
        "embedding_df = pd.read_parquet('/content/drive/MyDrive/deep_learning_project/data/embeddings/document_vector.parquet')\n",
        "\n",
        "# Check the embedding vectors dimension\n",
        "embedding_dim = len(embedding_df['document_vector'].iloc[0])\n",
        "\n",
        "# Mapping article_id -> embedding index\n",
        "article_to_index = {article_id: idx for idx, article_id in enumerate(embedding_df['article_id'])}\n",
        "\n",
        "# Initialisation of embedding matrix\n",
        "num_articles = len(article_to_index)\n",
        "embedding_matrix = np.zeros((num_articles, embedding_dim))\n",
        "\n",
        "# Puopulate the embedding matrix\n",
        "for idx, row in embedding_df.iterrows():\n",
        "    embedding_matrix[article_to_index[row['article_id']]] = np.array(row['document_vector'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Zapisz article_to_index\n",
        "with open('/content/drive/MyDrive/deep_learning_project/data/article_to_index.pkl', 'wb') as f:\n",
        "    pickle.dump(article_to_index, f)\n",
        "\n",
        "# Zapisz embedding_matrix w pliku npz\n",
        "np.save('/content/drive/MyDrive/deep_learning_project/data/embedding_matrix.npy', embedding_matrix)\n",
        "\n",
        "# Zapisz embedding_dim też gdzieś, np. w pickle:\n",
        "with open('/content/drive/MyDrive/deep_learning_project/data/embedding_dim.pkl', 'wb') as f:\n",
        "    pickle.dump(embedding_dim, f)"
      ],
      "metadata": {
        "id": "jasebw3iSepe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Sample 0.5% of each dataset to create smaller subsets\n",
        "# df_train_history = df_train_history.sample(frac=0.005, random_state=42)\n",
        "# df_train_behaviors = df_train_behaviors.sample(frac=0.005, random_state=42)\n",
        "# df_train_articles = df_train_articles.sample(frac=0.005, random_state=42)\n",
        "\n",
        "# df_valid_history = df_valid_history.sample(frac=0.005, random_state=42)\n",
        "# df_valid_behaviors = df_valid_behaviors.sample(frac=0.005, random_state=42)\n",
        "# df_valid_articles = df_valid_articles.sample(frac=0.005, random_state=42)\n",
        "\n",
        "# df_test_history = df_test_history.sample(frac=0.005, random_state=42)\n",
        "# df_test_behaviors = df_test_behaviors.sample(frac=0.005, random_state=42)\n",
        "# df_test_articles = df_test_articles.sample(frac=0.005, random_state=42)\n",
        "\n",
        "# # Subset the embedding dataframe\n",
        "# # embedding_df = embedding_df.sample(frac=0.005, random_state=42)\n",
        "\n",
        "# # # Recalculate the article-to-index mapping and embedding matrix for the smaller embedding_df\n",
        "# # article_to_index = {article_id: idx for idx, article_id in enumerate(embedding_df['article_id'])}\n",
        "\n",
        "# # # Reinitialize the embedding matrix\n",
        "# # num_articles = len(article_to_index)\n",
        "# # embedding_matrix = np.zeros((num_articles, embedding_dim))\n",
        "\n",
        "# # # Populate the new embedding matrix\n",
        "# # for idx, row in embedding_df.iterrows():\n",
        "# #     embedding_matrix[article_to_index[row['article_id']]] = np.array(row['document_vector'])\n",
        "\n",
        "# # # Your variables now refer to smaller subsets of the original data.\n"
      ],
      "metadata": {
        "id": "a_rfivFzV_YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here we define functions which are used to pre-process the input data**"
      ],
      "metadata": {
        "id": "pVjrJjlZiVOa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D92TNSRVM_so"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Function to transform the history of the user into tensor containing the articles embeddings\n",
        "def process_user_history(df_history, article_to_index, embedding_matrix, max_history_length):\n",
        "\n",
        "    # Initialize a list to store padded embeddings\n",
        "    article_embeddings = []\n",
        "\n",
        "    for article_ids in df_history['article_id_fixed']:\n",
        "        # Collect embeddings for valid article IDs\n",
        "        embeddings = [embedding_matrix[article_to_index[article_id]]\n",
        "                      for article_id in article_ids if article_id in article_to_index]\n",
        "\n",
        "        # Pad or truncate to the fixed history length\n",
        "        if len(embeddings) > max_history_length:\n",
        "            embeddings = embeddings[:max_history_length]\n",
        "        elif len(embeddings) < max_history_length:\n",
        "            embeddings += [np.zeros(embedding_matrix.shape[1])] * (max_history_length - len(embeddings))\n",
        "\n",
        "        article_embeddings.append(embeddings)\n",
        "\n",
        "    # Convert to a NumPy array and ensure correct dtype\n",
        "    padded_array = np.array(article_embeddings, dtype=np.float32)\n",
        "\n",
        "    # Map user to index\n",
        "    #user_id_to_index = {user_id: idx for idx, user_id in enumerate(df_history['user_id'].unique())}\n",
        "\n",
        "    return tf.convert_to_tensor(padded_array)#, user_id_to_index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def compute_article_age(df_behaviors, df_articles):\n",
        "    # Explode 'article_ids_inview' to have one article per row\n",
        "    df_behaviors_exploded = df_behaviors.explode('article_ids_inview')\n",
        "\n",
        "    # Merge with articles to get 'published_time', specify suffixes to avoid column name conflicts\n",
        "    df_merged = df_behaviors_exploded.merge(\n",
        "        df_articles[['article_id', 'published_time']],\n",
        "        left_on='article_ids_inview',\n",
        "        right_on='article_id',\n",
        "        how='left',\n",
        "        suffixes=('', '_article')  # Specifying suffixes\n",
        "    )\n",
        "\n",
        "    # Convert timestamps to datetime\n",
        "    df_merged['impression_time'] = pd.to_datetime(df_merged['impression_time'])\n",
        "    df_merged['published_time'] = pd.to_datetime(df_merged['published_time'])\n",
        "\n",
        "    # Compute article age in hours\n",
        "    df_merged['article_age'] = (df_merged['impression_time'] - df_merged['published_time']).dt.total_seconds() / 3600.0\n",
        "    df_merged['article_age'] = df_merged['article_age'].fillna(0)\n",
        "\n",
        "    # Handle missing 'article_ids_clicked' in test set\n",
        "    if 'article_ids_clicked' in df_merged.columns:\n",
        "        agg_dict = {\n",
        "            'user_id': 'first',\n",
        "            'article_id': 'first',  # From df_behaviors_exploded\n",
        "            'session_id': 'first',\n",
        "            'article_ids_inview': list,\n",
        "            'article_ids_clicked': 'first',\n",
        "            'article_age': list,\n",
        "            'impression_time': 'first',\n",
        "            'hour_of_day': 'first',\n",
        "            'day_of_week': 'first',\n",
        "        }\n",
        "    else:\n",
        "        agg_dict = {\n",
        "            'user_id': 'first',\n",
        "            'article_id': 'first',  # From df_behaviors_exploded\n",
        "            'session_id': 'first',\n",
        "            'article_ids_inview': list,\n",
        "            'article_age': list,\n",
        "            'impression_time': 'first',\n",
        "            'hour_of_day': 'first',\n",
        "            'day_of_week': 'first',\n",
        "        }\n",
        "\n",
        "    df_grouped = df_merged.groupby('impression_id').agg(agg_dict).reset_index()\n",
        "\n",
        "    return df_grouped"
      ],
      "metadata": {
        "id": "kJXQ3STUGQId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_session_time_features(df_behaviors):\n",
        "    df_behaviors['impression_time'] = pd.to_datetime(df_behaviors['impression_time'])\n",
        "    df_behaviors['hour_of_day'] = df_behaviors['impression_time'].dt.hour\n",
        "    df_behaviors['day_of_week'] = df_behaviors['impression_time'].dt.weekday  # 0=Monday, 6=Sunday\n",
        "    return df_behaviors"
      ],
      "metadata": {
        "id": "-BpATMLtGSZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_user_activity_features(df_behaviors):\n",
        "    df_behaviors = df_behaviors.sort_values(['user_id', 'impression_time'])\n",
        "    df_behaviors['time_since_last_session'] = df_behaviors.groupby('user_id')['impression_time'].diff().dt.total_seconds().fillna(0)\n",
        "    return df_behaviors"
      ],
      "metadata": {
        "id": "rzI-c4nTGWbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_or_truncate_list(lst, target_length, padding_value):\n",
        "    lst = list(lst)\n",
        "    if len(lst) > target_length:\n",
        "        return lst[:target_length]\n",
        "    else:\n",
        "        return lst + [padding_value] * (target_length - len(lst))"
      ],
      "metadata": {
        "id": "Vshe0lefGr6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_in_session_histories(df_behaviors):\n",
        "    # Sort by user_id, session_id, and impression_time\n",
        "    df_behaviors = df_behaviors.sort_values(['user_id', 'session_id', 'impression_time'])\n",
        "\n",
        "    # Initialize a dictionary to store in-session histories\n",
        "    in_session_histories = {}\n",
        "\n",
        "    # Group by session\n",
        "    grouped = df_behaviors.groupby(['user_id', 'session_id'])\n",
        "\n",
        "    # Iterate over each session\n",
        "    for (user_id, session_id), group in grouped:\n",
        "        viewed_articles = []\n",
        "        for idx, row in group.iterrows():\n",
        "            # Store the current viewed articles\n",
        "            in_session_histories[idx] = list(viewed_articles)\n",
        "\n",
        "            # Update the viewed_articles list with the current article\n",
        "            article_id = row['article_id']\n",
        "            if article_id is not None:\n",
        "                viewed_articles.append(article_id)\n",
        "    return in_session_histories"
      ],
      "metadata": {
        "id": "gMneUu8LOQ-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmVUFeh6M1qk"
      },
      "outputs": [],
      "source": [
        "def generate_session_labels(df_behaviors, article_to_index, embedding_matrix, max_articles_in_view=10, max_in_session_history=5, max_popularity_articles=10):\n",
        "    session_data = []\n",
        "\n",
        "    for _, row in df_behaviors.iterrows():\n",
        "        user_id = row['user_id']\n",
        "        impression_id = row['impression_id']\n",
        "        articles_in_view = np.array(row['article_ids_inview'])\n",
        "        articles_clicked = set(row['article_ids_clicked']) if row['article_ids_clicked'] is not None else set()\n",
        "        in_session_history = row['in_session_history']\n",
        "\n",
        "        # Generate embeddings for in-session history\n",
        "        in_session_embeddings = [\n",
        "            embedding_matrix[article_to_index.get(article_id, 0)]\n",
        "            for article_id in in_session_history\n",
        "        ]\n",
        "\n",
        "        # Pad or truncate to max_in_session_history\n",
        "        in_session_embeddings = pad_or_truncate_list(in_session_embeddings, max_in_session_history, np.zeros(embedding_matrix.shape[1]))\n",
        "\n",
        "        # Popularity articles embeddings\n",
        "        popularity_articles = row['popularity_articles']\n",
        "        popularity_embeddings = [\n",
        "            embedding_matrix[article_to_index.get(article_id, 0)]\n",
        "            for article_id in popularity_articles\n",
        "        ]\n",
        "        popularity_embeddings = pad_or_truncate_list(popularity_embeddings, max_popularity_articles, np.zeros(embedding_matrix.shape[1]))\n",
        "\n",
        "        # Existing code for article embeddings and labels\n",
        "        embeddings = [\n",
        "            embedding_matrix[article_to_index.get(article_id, 0)]\n",
        "            for article_id in articles_in_view\n",
        "        ]\n",
        "        embeddings = pad_or_truncate_list(embeddings, max_articles_in_view, np.zeros(embedding_matrix.shape[1]))\n",
        "\n",
        "        labels = np.isin(articles_in_view, list(articles_clicked)).astype(int)\n",
        "        labels = pad_or_truncate_list(labels, max_articles_in_view, 0)\n",
        "\n",
        "        session_data.append({\n",
        "            'user_id': user_id,\n",
        "            'impression_id': impression_id,\n",
        "            'article_embeddings': embeddings,\n",
        "            'in_session_embeddings': in_session_embeddings,\n",
        "            'popularity_embeddings': popularity_embeddings,\n",
        "            'labels': labels,\n",
        "            # Include other features as needed\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(session_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_test_sessions(df_behaviors, article_to_index, embedding_matrix, max_articles_in_view=10, max_in_session_history=5, max_popularity_articles=10):\n",
        "    session_data = []\n",
        "\n",
        "    for _, row in df_behaviors.iterrows():\n",
        "        user_id = row['user_id']\n",
        "        impression_id = row['impression_id']\n",
        "        articles_in_view = np.array(row['article_ids_inview'])\n",
        "\n",
        "        # In-session history embeddings\n",
        "        in_session_history = row.get('in_session_history', [])\n",
        "        in_session_embeddings = [\n",
        "            embedding_matrix[article_to_index.get(article_id, 0)]\n",
        "            for article_id in in_session_history\n",
        "        ]\n",
        "        in_session_embeddings = pad_or_truncate_list(in_session_embeddings, max_in_session_history, np.zeros(embedding_matrix.shape[1]))\n",
        "\n",
        "        # Popularity articles embeddings\n",
        "        popularity_articles = row.get('popularity_articles', [])\n",
        "        popularity_embeddings = [\n",
        "            embedding_matrix[article_to_index.get(article_id, 0)]\n",
        "            for article_id in popularity_articles\n",
        "        ]\n",
        "        popularity_embeddings = pad_or_truncate_list(popularity_embeddings, max_popularity_articles, np.zeros(embedding_matrix.shape[1]))\n",
        "\n",
        "        # Article embeddings\n",
        "        embeddings = [\n",
        "            embedding_matrix[article_to_index.get(article_id, 0)]\n",
        "            for article_id in articles_in_view\n",
        "        ]\n",
        "        embeddings = pad_or_truncate_list(embeddings, max_articles_in_view, np.zeros(embedding_matrix.shape[1]))\n",
        "\n",
        "        session_data.append({\n",
        "            'user_id': user_id,\n",
        "            'impression_id': impression_id,\n",
        "            'article_embeddings': embeddings,\n",
        "            'in_session_embeddings': in_session_embeddings,\n",
        "            'popularity_embeddings': popularity_embeddings,\n",
        "            # No labels since we don't have 'article_ids_clicked'\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(session_data)"
      ],
      "metadata": {
        "id": "v2gIzPRy5wPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_popularity_features(df_behaviors, popularity_window_hours, top_n=10):\n",
        "    \"\"\"\n",
        "    Adds a 'popularity_articles' column to df_behaviors, which lists the top N articles\n",
        "    popular in the last 'popularity_window_hours' before each impression.\n",
        "    \"\"\"\n",
        "    # Convert 'impression_time' to datetime\n",
        "    df_behaviors['impression_time'] = pd.to_datetime(df_behaviors['impression_time'])\n",
        "\n",
        "    # Create a DataFrame 'df_views' containing each viewed article with the corresponding time\n",
        "    df_views = df_behaviors[['impression_time', 'article_id']].dropna(subset=['article_id'])\n",
        "\n",
        "    # Set 'impression_time' as the index and sort it\n",
        "    df_views.set_index('impression_time', inplace=True)\n",
        "    df_views.sort_index(inplace=True)\n",
        "\n",
        "    # Initialize a cache dictionary to store popularity for each time period\n",
        "    popularity_cache = {}\n",
        "\n",
        "    # Time window in Timedelta\n",
        "    time_window = pd.Timedelta(hours=popularity_window_hours)\n",
        "\n",
        "    # Iterate through unique impression times\n",
        "    impression_times = df_behaviors['impression_time'].unique()\n",
        "\n",
        "    for time in impression_times:\n",
        "        if time in popularity_cache:\n",
        "            continue\n",
        "        # Define the time window\n",
        "        start_time = time - time_window\n",
        "        # Ensure that start_time and time are Timestamps\n",
        "        start_time = pd.to_datetime(start_time)\n",
        "        time = pd.to_datetime(time)\n",
        "        # Filter views in the time window\n",
        "        views_in_window = df_views.loc[start_time:time]\n",
        "        # Count article views\n",
        "        article_counts = views_in_window['article_id'].value_counts()\n",
        "        # Get the top N most popular articles\n",
        "        popular_articles = article_counts.head(top_n).index.tolist()\n",
        "        # Store results in the cache\n",
        "        popularity_cache[time] = popular_articles\n",
        "\n",
        "    # Map popularity to df_behaviors\n",
        "    df_behaviors['popularity_articles'] = df_behaviors['impression_time'].map(popularity_cache)\n",
        "\n",
        "    # Replace NaN with an empty list if there are no popular articles\n",
        "    df_behaviors['popularity_articles'] = df_behaviors['popularity_articles'].apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "    return df_behaviors"
      ],
      "metadata": {
        "id": "5JVQ1gpuOKlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM763WuUSrYE"
      },
      "outputs": [],
      "source": [
        "def create_tf_dataset(df_labeled_sessions, user_id_to_index, batch_size):\n",
        "    user_indices = df_labeled_sessions['user_id'].map(user_id_to_index).fillna(0).astype(int).to_numpy()\n",
        "    article_embeddings = np.stack(df_labeled_sessions['article_embeddings'].to_numpy())\n",
        "    in_session_embeddings = np.stack(df_labeled_sessions['in_session_embeddings'].to_numpy())\n",
        "    popularity_embeddings = np.stack(df_labeled_sessions['popularity_embeddings'].to_numpy())\n",
        "    labels = np.stack(df_labeled_sessions['labels'].to_numpy())\n",
        "\n",
        "    \"\"\"\n",
        "    # Convert to TensorFlow tensors and ensure they are on the CPU\n",
        "    with tf.device('/CPU:0'):  # Force tensors to reside on the CPU\n",
        "        user_indices = tf.convert_to_tensor(user_indices, dtype=tf.int32)\n",
        "        article_embeddings = tf.convert_to_tensor(article_embeddings, dtype=tf.float32)\n",
        "        in_session_embeddings = tf.convert_to_tensor(in_session_embeddings, dtype=tf.float32)\n",
        "        labels = tf.convert_to_tensor(labels, dtype=tf.int32) # Assuming labels are integers\n",
        "    \"\"\"\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        ((user_indices, article_embeddings, in_session_embeddings, popularity_embeddings), labels)\n",
        "    ).batch(batch_size)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tf_dataset_for_prediction(df_sessions, user_id_to_index, batch_size):\n",
        "    user_indices = df_sessions['user_id'].map(user_id_to_index).astype(int).to_numpy()\n",
        "    article_embeddings = np.stack(df_sessions['article_embeddings'].to_numpy())\n",
        "    in_session_embeddings = np.stack(df_sessions['in_session_embeddings'].to_numpy())\n",
        "    popularity_embeddings = np.stack(df_sessions['popularity_embeddings'].to_numpy())\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        (user_indices, article_embeddings, in_session_embeddings, popularity_embeddings)\n",
        "    ))\n",
        "\n",
        "    return dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "GgblNj2a52so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here we define the model class and its sub-classes**"
      ],
      "metadata": {
        "id": "V9MjwTfHihiH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Epdmy7FQIjAH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.layers import LayerNormalization, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "class UserEncoder(Model):\n",
        "    def __init__(self, embedding_dim, num_heads, attention_dim, dropout_rate=0.2, **kwargs):\n",
        "        super(UserEncoder, self).__init__(**kwargs)\n",
        "\n",
        "        # Self-attention layer\n",
        "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
        "        self.layer_norm1 = layers.LayerNormalization()\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "\n",
        "        # Additive attention layer\n",
        "        self.additive_attention_dense = layers.Dense(embedding_dim, activation='tanh')\n",
        "        self.layer_norm2 = layers.LayerNormalization()\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "\n",
        "        # Extra dense layer\n",
        "        self.attention_score_dense = layers.Dense(1)\n",
        "        self.softmax = layers.Softmax(axis=1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        # Self-attention layer\n",
        "        attention_output = self.multi_head_attention(inputs, inputs)\n",
        "        attention_output = self.layer_norm1(attention_output)\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "\n",
        "        # Additive attention layer\n",
        "        additive_attention_output = self.additive_attention_dense(attention_output)\n",
        "        additive_attention_output = self.layer_norm2(additive_attention_output)\n",
        "        additive_attention_output = self.dropout2(additive_attention_output)\n",
        "\n",
        "        # Dense layer\n",
        "        attention_scores = self.attention_score_dense(additive_attention_output)\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "\n",
        "        weighted_output = tf.reduce_sum(attention_output * attention_weights, axis=1)\n",
        "        return weighted_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class InSessionEncoder(Model):\n",
        "    def __init__(self, embedding_dim, num_heads, **kwargs):\n",
        "        super(InSessionEncoder, self).__init__(**kwargs)\n",
        "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
        "        self.layer_norm = layers.LayerNormalization()\n",
        "        self.dropout = layers.Dropout(0.2)\n",
        "        self.output_layer = layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attention_output = self.multi_head_attention(inputs, inputs)\n",
        "        attention_output = self.layer_norm(attention_output)\n",
        "        attention_output = self.dropout(attention_output)\n",
        "        # Aggregate the outputs\n",
        "        in_session_representation = tf.reduce_mean(attention_output, axis=1)\n",
        "        in_session_representation = self.output_layer(in_session_representation)\n",
        "        return in_session_representation"
      ],
      "metadata": {
        "id": "PKBxQM7POp_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PopularityEncoder(Model):\n",
        "    def __init__(self, embedding_dim, num_heads, **kwargs):\n",
        "        super(PopularityEncoder, self).__init__(**kwargs)\n",
        "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
        "        self.layer_norm = layers.LayerNormalization()\n",
        "        self.dropout = layers.Dropout(0.2)\n",
        "        self.output_layer = layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs: tensor with dimensions: (batch_size, max_popularity_articles, embedding_dim)\n",
        "        attention_output = self.multi_head_attention(inputs, inputs)\n",
        "        attention_output = self.layer_norm(attention_output)\n",
        "        attention_output = self.dropout(attention_output)\n",
        "        # Aggregation of outputs\n",
        "        popularity_representation = tf.reduce_mean(attention_output, axis=1)\n",
        "        popularity_representation = self.output_layer(popularity_representation)\n",
        "        return popularity_representation"
      ],
      "metadata": {
        "id": "9CDBULAmUU5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFdvrjB5OP5L"
      },
      "outputs": [],
      "source": [
        "class ClickPredictor(Model):\n",
        "    def __init__(self, input_dim, **kwargs):\n",
        "        super(ClickPredictor, self).__init__(**kwargs)\n",
        "        self.dense1 = layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))\n",
        "        self.dropout1 = layers.Dropout(0.2)\n",
        "        self.dense2 = layers.Dense(128, activation='relu')\n",
        "        self.dropout2 = layers.Dropout(0.2)\n",
        "        self.dense3 = layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dropout2(x)\n",
        "        click_probability = self.dense3(x)\n",
        "        return click_probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XcPxsoUOeAW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.metrics import AUC\n",
        "\n",
        "class NewsRecommendationModel(Model):\n",
        "    def __init__(self, user_histories_tensor, embedding_dim, num_heads, attention_dim, **kwargs):\n",
        "        super(NewsRecommendationModel, self).__init__(**kwargs)\n",
        "        self.user_histories_tensor = user_histories_tensor\n",
        "        self.user_encoder = UserEncoder(embedding_dim=embedding_dim, num_heads=num_heads, attention_dim=attention_dim)\n",
        "        self.in_session_encoder = InSessionEncoder(embedding_dim=embedding_dim, num_heads=num_heads)\n",
        "        self.popularity_encoder = PopularityEncoder(embedding_dim=embedding_dim, num_heads=num_heads)\n",
        "\n",
        "        # Self-attention layer\n",
        "        self.self_attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
        "        self.layer_norm = LayerNormalization()\n",
        "        self.dropout = Dropout(0.2)\n",
        "\n",
        "        # Dense layer for user representation projection\n",
        "        self.user_projection = layers.Dense(embedding_dim, activation='relu')\n",
        "\n",
        "        self.click_predictor = ClickPredictor(input_dim=embedding_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        user_indices, article_embeddings, in_session_embeddings, popularity_embeddings = inputs\n",
        "\n",
        "        # User representation\n",
        "        user_histories = tf.gather(self.user_histories_tensor, user_indices)\n",
        "        user_representation = self.user_encoder(user_histories)\n",
        "\n",
        "        # In-session representation\n",
        "        in_session_representation = self.in_session_encoder(in_session_embeddings)\n",
        "\n",
        "        # Popularity representation\n",
        "        popularity_representation = self.popularity_encoder(popularity_embeddings)\n",
        "\n",
        "        # Combining user representations\n",
        "        combined_user_representation = tf.concat([user_representation, in_session_representation, popularity_representation], axis=-1)  # Shape: (batch_size, combined_dim)\n",
        "\n",
        "        # Projection to embedding_dim\n",
        "        combined_user_representation = self.user_projection(combined_user_representation)  # Shape: (batch_size, embedding_dim)\n",
        "\n",
        "        # Preparing data for attention\n",
        "        batch_size = tf.shape(article_embeddings)[0]\n",
        "        num_articles = tf.shape(article_embeddings)[1]\n",
        "\n",
        "        # Expanding dimensions and concatenation\n",
        "        user_representation_expanded = tf.expand_dims(combined_user_representation, axis=1)  # Shape: (batch_size, 1, embedding_dim)\n",
        "        sequence = tf.concat([user_representation_expanded, article_embeddings], axis=1)  # Shape: (batch_size, num_articles + 1, embedding_dim)\n",
        "\n",
        "        # Applying self-attention\n",
        "        attention_output = self.self_attention(sequence, sequence)\n",
        "        attention_output = self.dropout(attention_output)\n",
        "        attention_output = self.layer_norm(sequence + attention_output)\n",
        "\n",
        "        # Extracting article representations after attention (skipping the first element, which is the user representation)\n",
        "        article_attention_output = attention_output[:, 1:, :]  # Shape: (batch_size, num_articles, embedding_dim)\n",
        "\n",
        "        # Flattening and click prediction for each article\n",
        "        article_flat = tf.reshape(article_attention_output, [-1, embedding_dim])  # Shape: (batch_size * num_articles, embedding_dim)\n",
        "        click_probabilities_flat = self.click_predictor(article_flat)  # Shape: (batch_size * num_articles, 1)\n",
        "        click_probabilities = tf.reshape(click_probabilities_flat, [batch_size, num_articles])  # Shape: (batch_size, num_articles)\n",
        "\n",
        "        return click_probabilities\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here we define all the hyperparameters used in the model**"
      ],
      "metadata": {
        "id": "fFrp2S4_qw4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DEFINE ALL THE HYPERPARAMETERS\n",
        "embedding_dim = 300             # Dimension of the article embedding vectors\n",
        "num_heads = 16                  # Number of attention heads in the attention layer\n",
        "attention_dim = 32              # Dimension of the attention space\n",
        "batch_size = 64                 # Number of samples used in each training iteration\n",
        "epochs_num = 16                 # Number of times the model will iterate over the entire training dataset\n",
        "initial_learning_rate=0.001     # Initial value of learning rate (learning rate is dynamically set by the scheduler)\n",
        "max_history_length = 32         # Maximum length of user history considered by the model\n",
        "max_articles_in_view = 10       # Maximum number of articles in a user's viewing session (if applicable)\n",
        "popularity_window_hours = 48    # Number of hours to consider for popularity calculation\n",
        "top_N_popular_articles = 10     # Number of top popular articles to consider"
      ],
      "metadata": {
        "id": "5nnyPnJzpymA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here we pass the input data through the predefined functions (input data pre-processing)**"
      ],
      "metadata": {
        "id": "gypq3MTyi4qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/drive/MyDrive/deep_learning_project/data/article_to_index.pkl', 'rb') as f:\n",
        "    article_to_index = pickle.load(f)\n",
        "\n",
        "embedding_matrix = np.load('/content/drive/MyDrive/deep_learning_project/data/embedding_matrix.npy')\n",
        "\n",
        "with open('/content/drive/MyDrive/deep_learning_project/data/embedding_dim.pkl', 'rb') as f:\n",
        "    embedding_dim = pickle.load(f)"
      ],
      "metadata": {
        "id": "zoGP-QSSSrow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/deep_learning_project/data/user_id_to_index.pkl', 'rb') as f:\n",
        "    user_id_to_index = pickle.load(f)"
      ],
      "metadata": {
        "id": "aUIc3cNZUwgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_behaviors = compute_session_time_features(df_train_behaviors)\n",
        "df_train_behaviors = compute_user_activity_features(df_train_behaviors)\n",
        "df_train_behaviors = compute_article_age(df_train_behaviors, df_train_articles)\n",
        "df_train_behaviors = compute_popularity_features(df_train_behaviors, popularity_window_hours, top_N_popular_articles)\n",
        "\n",
        "# Zapisz gotowy df_behaviors do pliku:\n",
        "df_train_behaviors.to_parquet('/content/drive/MyDrive/deep_learning_project/data/train_full_preprocessed.parquet')"
      ],
      "metadata": {
        "id": "K1FFMmVBWkcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid_behaviors = compute_session_time_features(df_valid_behaviors)\n",
        "df_valid_behaviors = compute_user_activity_features(df_valid_behaviors)\n",
        "df_valid_behaviors = compute_article_age(df_valid_behaviors, df_valid_articles)\n",
        "df_valid_behaviors = compute_popularity_features(df_valid_behaviors, popularity_window_hours, top_N_popular_articles)\n",
        "\n",
        "# Zapisz gotowy df_behaviors do pliku:\n",
        "df_valid_behaviors.to_parquet('/content/drive/MyDrive/deep_learning_project/data/valid_full_preprocessed.parquet')"
      ],
      "metadata": {
        "id": "I7ByH_NaXZIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\"\"\"\n",
        "def prepare_data(df_history, df_behaviors, df_articles, article_to_index, embedding_matrix, max_history_length, popularity_window_hours, top_N_popular_articles, user_id_to_index, is_training=True):\n",
        "\n",
        "    # Compute temporal features\n",
        "    df_behaviors = compute_session_time_features(df_behaviors)\n",
        "    df_behaviors = compute_user_activity_features(df_behaviors)\n",
        "    df_behaviors = compute_article_age(df_behaviors, df_articles)\n",
        "\n",
        "    # Compute and add in-session history\n",
        "    in_session_histories = build_in_session_histories(df_behaviors)\n",
        "    df_behaviors['in_session_history'] = df_behaviors.index.map(in_session_histories)\n",
        "\n",
        "    # Compute popularity features\n",
        "    df_behaviors = compute_popularity_features(df_behaviors, popularity_window_hours, top_N_popular_articles)\n",
        "\n",
        "    # Prepare user histories\n",
        "    #user_histories_tensor, user_id_to_index = process_user_history(\n",
        "    #    df_history, article_to_index, embedding_matrix, max_history_length, user_id_to_index\n",
        "    #)\n",
        "    user_histories_tensor = process_user_history(\n",
        "        df_history, article_to_index, embedding_matrix, max_history_length\n",
        "    )\n",
        "\n",
        "    # Generate session labels including temporal features\n",
        "    df_labeled_sessions = generate_session_labels(df_behaviors, article_to_index, embedding_matrix)\n",
        "\n",
        "    # Create dataset including temporal features\n",
        "    dataset = create_tf_dataset(df_labeled_sessions, user_id_to_index, batch_size=32)\n",
        "\n",
        "    return dataset, user_histories_tensor, user_id_to_index\n",
        "    \"\"\"\n",
        "\n",
        "def prepare_data(df_history, df_behaviors, df_articles, article_to_index, embedding_matrix, max_history_length, user_id_to_index, is_training=True):\n",
        "    # Zakładamy, że df_behaviors jest już wstępnie przetworzony i ma kolumny popularity_articles, hour_of_day, day_of_week, article_age itp.\n",
        "\n",
        "    in_session_histories = build_in_session_histories(df_behaviors)\n",
        "    df_behaviors['in_session_history'] = df_behaviors.index.map(in_session_histories)\n",
        "\n",
        "    # Nie wywołujemy compute_popularity_features, compute_session_time_features, itd. bo już zostały zrobione globalnie\n",
        "\n",
        "    # Teraz process_user_history (zakładając że user_id_to_index i reszta jest globalna lub przekazana)\n",
        "    user_histories_tensor = process_user_history(\n",
        "        df_history, article_to_index, embedding_matrix, max_history_length\n",
        "    )\n",
        "\n",
        "    # Generate session labels\n",
        "    df_labeled_sessions = generate_session_labels(df_behaviors, article_to_index, embedding_matrix)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = create_tf_dataset(df_labeled_sessions, user_id_to_index, batch_size=32)\n",
        "\n",
        "    return dataset, user_histories_tensor\n",
        "\n",
        "# Prepare datasets\n",
        "#train_dataset, train_user_histories_tensor, user_id_to_index = prepare_data(df_train_history, df_train_behaviors, df_train_articles, article_to_index, embedding_matrix, max_history_length, popularity_window_hours, top_N_popular_articles, user_id_to_index, is_training=True)\n",
        "#validation_dataset, _, _ = prepare_data(df_valid_history, df_valid_behaviors, df_valid_articles, article_to_index, embedding_matrix, max_history_length, popularity_window_hours, top_N_popular_articles, user_id_to_index, is_training=False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BiaH7t4ptckr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here we train and test the model**"
      ],
      "metadata": {
        "id": "Yy3ifD4ekEHc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pag0uwH6PUJc",
        "outputId": "36ff035b-8ed2-4b2b-8c72-1ae766b3f53d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 51ms/step - auc: 0.6720 - loss: 0.4334 - val_auc: 0.7316 - val_loss: 0.2727 - learning_rate: 9.9189e-04\n",
            "Epoch 2/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 39ms/step - auc: 0.7400 - loss: 0.2663 - val_auc: 0.7287 - val_loss: 0.2625 - learning_rate: 9.8384e-04\n",
            "Epoch 3/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 41ms/step - auc: 0.7485 - loss: 0.2581 - val_auc: 0.7237 - val_loss: 0.2635 - learning_rate: 9.7586e-04\n",
            "Epoch 4/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 41ms/step - auc: 0.7524 - loss: 0.2564 - val_auc: 0.7201 - val_loss: 0.2644 - learning_rate: 9.6795e-04\n",
            "Epoch 5/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 41ms/step - auc: 0.7556 - loss: 0.2554 - val_auc: 0.7169 - val_loss: 0.2685 - learning_rate: 9.6010e-04\n",
            "Epoch 6/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 41ms/step - auc: 0.7586 - loss: 0.2545 - val_auc: 0.7201 - val_loss: 0.2657 - learning_rate: 9.5231e-04\n",
            "Epoch 7/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.7610 - loss: 0.2538 - val_auc: 0.7234 - val_loss: 0.2660 - learning_rate: 9.4458e-04\n",
            "Epoch 8/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 41ms/step - auc: 0.7631 - loss: 0.2532 - val_auc: 0.7209 - val_loss: 0.2634 - learning_rate: 9.3692e-04\n",
            "Epoch 9/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 41ms/step - auc: 0.7657 - loss: 0.2523 - val_auc: 0.7136 - val_loss: 0.2674 - learning_rate: 9.2932e-04\n",
            "Epoch 10/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.7672 - loss: 0.2519 - val_auc: 0.7027 - val_loss: 0.2682 - learning_rate: 9.2178e-04\n",
            "Epoch 11/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.7684 - loss: 0.2516 - val_auc: 0.7177 - val_loss: 0.2633 - learning_rate: 9.1431e-04\n",
            "Epoch 12/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 41ms/step - auc: 0.7701 - loss: 0.2509 - val_auc: 0.7113 - val_loss: 0.2713 - learning_rate: 9.0689e-04\n",
            "Epoch 13/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.7726 - loss: 0.2503 - val_auc: 0.7107 - val_loss: 0.2696 - learning_rate: 8.9954e-04\n",
            "Epoch 14/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.7744 - loss: 0.2496 - val_auc: 0.7020 - val_loss: 0.2805 - learning_rate: 8.9224e-04\n",
            "Epoch 15/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.7783 - loss: 0.2485 - val_auc: 0.7038 - val_loss: 0.2774 - learning_rate: 8.8500e-04\n",
            "Epoch 16/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 41ms/step - auc: 0.7805 - loss: 0.2478 - val_auc: 0.7208 - val_loss: 0.2679 - learning_rate: 8.7782e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7820eb3c0130>"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Create a model instance\n",
        "model = NewsRecommendationModel(\n",
        "                                user_histories_tensor=train_user_histories_tensor,\n",
        "                                embedding_dim=embedding_dim,\n",
        "                                num_heads=num_heads,\n",
        "                                attention_dim=attention_dim\n",
        "                              )\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "# Define the scheduler (to dynamically set the optimal learning rate)\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=initial_learning_rate,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "\n",
        "# Create optimizer using above scheduler\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "# Define the callback (reduces the learning rate when the validation loss stops to decrease)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.001)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer,  # Use the optimizer instance\n",
        "              loss=loss_fn,\n",
        "              metrics=[tf.keras.metrics.AUC(name=\"auc\")])\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=epochs_num,\n",
        "    callbacks=[reduce_lr]\n",
        ")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Określ rozmiar chunków, np. 100_000 wierszy na część (dostosuj do dostępnej pamięci)\n",
        "chunk_size = 25_000\n",
        "num_chunks = len(df_train_behaviors) // chunk_size + int(len(df_train_behaviors) % chunk_size != 0)\n",
        "\n",
        "for i in range(num_chunks):\n",
        "    start_idx = i * chunk_size\n",
        "    end_idx = min((i+1)*chunk_size, len(df_train_behaviors))\n",
        "\n",
        "    # Wytnij fragment z behaviors\n",
        "    df_beh_chunk = df_train_behaviors.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "    # Aby poprawnie wyciąć fragment z history, musisz wybrać wiersze dla użytkowników występujących w df_beh_chunk\n",
        "    # Jeżeli 'user_id' jest kluczem łączącym z history, możesz go użyć:\n",
        "    user_ids_in_chunk = df_beh_chunk['user_id'].unique()\n",
        "    df_hist_chunk = df_train_history[df_train_history['user_id'].isin(user_ids_in_chunk)].copy()\n",
        "    # Podobnie jeśli potrzebne są artykuły, to zwykle articles jest pełne i to jest metadane,\n",
        "    # więc w wielu przypadkach nie dzielimy articles, a używamy pełnego zbioru artykułów.\n",
        "    # Ale jeśli chcesz też dzielić articles, to wybierz np. artykuły pojawiające się w tym chunku:\n",
        "    # article_ids_in_chunk = np.unique(np.concatenate(df_beh_chunk['article_ids_inview'].values))\n",
        "    # df_articles_chunk = df_full_articles[df_full_articles['article_id'].isin(article_ids_in_chunk)].copy()\n",
        "    # Jeśli artykuły są globalne i potrzebne w całości, pomiń ten krok i zawsze używaj pełnych articles.\n",
        "\n",
        "    # Zapisz część do parquet\n",
        "    df_beh_chunk.to_parquet(f'/content/drive/MyDrive/deep_learning_project/data/train/train_behaviors_part{i+1}.parquet')\n",
        "    df_hist_chunk.to_parquet(f'/content/drive/MyDrive/deep_learning_project/data/train/train_history_part{i+1}.parquet')\n",
        "\n",
        "    # Jeśli zdecydowałeś się też dzielić articles:\n",
        "    # df_articles_chunk.to_parquet(f'/content/drive/MyDrive/deep_learning_project/data/train/train_articles_part{i+1}.parquet')\n",
        "\n",
        "# Po tym etapie masz wiele części train_behaviors_partX.parquet i train_history_partX.parquet.\n",
        "# Możesz do nich odwołać się w liście train_parts."
      ],
      "metadata": {
        "id": "-Ua8T7mzY5oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "import os\n",
        "\n",
        "# Tworzymy model raz\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "lr_schedule = ExponentialDecay(\n",
        "    initial_learning_rate=initial_learning_rate,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9\n",
        ")\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "# Na tym etapie nie mamy jeszcze user_histories_tensor, bo weźmiemy go z pierwszej partii danych\n",
        "# Zrobimy to w pętli, inicjalizując model dopiero po wczytaniu pierwszego chunka\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
        "\n",
        "# Zakładamy że mamy listę plików z podzielonymi danymi treningowymi i walidacyjnymi:\n",
        "train_parts = [\n",
        "    '/content/drive/MyDrive/deep_learning_project/data/train/train_behaviors_part1.parquet',\n",
        "    '/content/drive/MyDrive/deep_learning_project/data/train/train_behaviors_part2.parquet',\n",
        "    '/content/drive/MyDrive/deep_learning_project/data/train/train_behaviors_part3.parquet',\n",
        "    '/content/drive/MyDrive/deep_learning_project/data/train/train_behaviors_part4.parquet',\n",
        "    '/content/drive/MyDrive/deep_learning_project/data/train/train_behaviors_part5.parquet',\n",
        "    '/content/drive/MyDrive/deep_learning_project/data/train/train_behaviors_part6.parquet',\n",
        "    '/content/drive/MyDrive/deep_learning_project/data/train/train_behaviors_part7.parquet',\n",
        "    '/content/drive/MyDrive/deep_learning_project/data/train/train_behaviors_part8.parquet',\n",
        "    '/content/drive/MyDrive/deep_learning_project/data/train/train_behaviors_part9.parquet',\n",
        "    '/content/drive/MyDrive/deep_learning_project/data/train/train_behaviors_part10.parquet',\n",
        "]\n",
        "\n",
        "valid_behaviors_path = '/content/drive/MyDrive/deep_learning_project/data/valid_full_preprocessed.parquet'\n",
        "valid_history_path = '/content/drive/MyDrive/deep_learning_project/data/validation/history.parquet'\n",
        "valid_articles_path = '/content/drive/MyDrive/deep_learning_project/data/validation/articles.parquet'\n",
        "\n",
        "# Najpierw trenujemy na pierwszej części aby zainicjalizować model\n",
        "df_train_behaviors_part1 = pd.read_parquet(train_parts[0])\n",
        "df_train_history_part1 = pd.read_parquet(train_parts[0].replace('behaviors','history'))\n",
        "df_train_articles = pd.read_parquet('/content/drive/MyDrive/deep_learning_project/data/train/articles.parquet')\n",
        "# zakładam że articles nie jest dzielone, bo to metadane wszystkich artykułów\n",
        "\n",
        "train_dataset_part1, train_user_histories_tensor_part1 = prepare_data(\n",
        "    df_train_history_part1, df_train_behaviors_part1, df_train_articles,\n",
        "    article_to_index, embedding_matrix, max_history_length, user_id_to_index, is_training=True\n",
        ")\n",
        "\n",
        "# Zainicjalizuj model\n",
        "model = NewsRecommendationModel(\n",
        "    user_histories_tensor=train_user_histories_tensor_part1,\n",
        "    embedding_dim=embedding_dim,\n",
        "    num_heads=num_heads,\n",
        "    attention_dim=attention_dim\n",
        ")\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=[tf.keras.metrics.AUC(name=\"auc\")])\n",
        "\n",
        "# Trenuj na pierwszej części\n",
        "print(\"Training on part: 1\")\n",
        "model.fit(train_dataset_part1, epochs=10)  # Możesz ustawić epochs inaczej, albo loopować.\n",
        "\n",
        "# Dla kolejnych części aktualizujemy user_histories_tensor i kontynuujemy trening\n",
        "i=1\n",
        "for part_path in train_parts[1:]:\n",
        "    i+=1\n",
        "    df_train_behaviors_part = pd.read_parquet(part_path)\n",
        "    df_train_history_part = pd.read_parquet(part_path.replace('behaviors','history'))\n",
        "\n",
        "    train_dataset_part, train_user_histories_tensor_part = prepare_data(\n",
        "        df_train_history_part, df_train_behaviors_part, df_train_articles,\n",
        "        article_to_index, embedding_matrix, max_history_length, user_id_to_index, is_training=True\n",
        "    )\n",
        "\n",
        "    # Zaktualizuj user_histories_tensor w modelu\n",
        "    model.user_histories_tensor = train_user_histories_tensor_part\n",
        "\n",
        "    print(f\"Training on part: {i}\")\n",
        "    model.fit(train_dataset_part, epochs=10)  # kontynuujesz trening"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "bIYHqV63YMmt",
        "outputId": "be780f1b-e7e3-432d-bd10-95dd78d46c4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InternalError",
          "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-e2183a016a15>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# zakładam że articles nie jest dzielone, bo to metadane wszystkich artykułów\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m train_dataset_part1, train_user_histories_tensor_part1 = prepare_data(\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mdf_train_history_part1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train_behaviors_part1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train_articles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0marticle_to_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_history_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_id_to_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-117-4afb4626cf8f>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(df_history, df_behaviors, df_articles, article_to_index, embedding_matrix, max_history_length, user_id_to_index, is_training)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Create dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tf_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_labeled_sessions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_id_to_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_histories_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-105-4df4918d5d9e>\u001b[0m in \u001b[0;36mcreate_tf_dataset\u001b[0;34m(df_labeled_sessions, user_id_to_index, batch_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \"\"\"\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     dataset = tf.data.Dataset.from_tensor_slices(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_session_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpopularity_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     ).batch(batch_size)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m     \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_tensor_slices_op.py\u001b[0m in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_from_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_TensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_tensor_slices_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    132\u001b[0m           \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m           normalized_components.append(\n\u001b[0;32m--> 134\u001b[0;31m               ops.convert_to_tensor(t, name=\"component_%d\" % i, dtype=dtype))\n\u001b[0m\u001b[1;32m    135\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_as\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    711\u001b[0m   \u001b[0;31m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m   \u001b[0mpreferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreferred_dtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m   return tensor_conversion_registry.convert(\n\u001b[0m\u001b[1;32m    714\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_result_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_tensor_conversion.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m   \"\"\"\n\u001b[0;32m--> 276\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    277\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m   const_tensor = ops._create_graph_constant(  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m ) -> ops._EagerTensorBase:\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Po zakończeniu treningu na wszystkich częściach, walidacja\n",
        "df_valid_behaviors = pd.read_parquet('/content/drive/MyDrive/deep_learning_project/data/valid_full_preprocessed.parquet')\n",
        "df_valid_history = pd.read_parquet(valid_history_path)\n",
        "df_valid_articles = pd.read_parquet(valid_articles_path)\n",
        "\n",
        "valid_dataset, valid_user_histories_tensor = prepare_data(\n",
        "    df_valid_history, df_valid_behaviors, df_valid_articles,\n",
        "    article_to_index, embedding_matrix, max_history_length, user_id_to_index, is_training=False\n",
        ")\n",
        "model.user_histories_tensor = valid_user_histories_tensor\n",
        "\n",
        "model.evaluate(valid_dataset)"
      ],
      "metadata": {
        "id": "sN4Yb6G7teuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "aTJISifFd_K1",
        "outputId": "1cdf7554-6406-4890-d645-51421d0b5ff6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\npredictions = model.predict(test_dataset)\\nlabels = np.concatenate([y for x, y in test_dataset], axis=0)\\n\\n# Flatten arrays\\npredictions_flat = predictions.flatten()\\nlabels_flat = labels.flatten()\\n\\n# Compute AUC\\nauc_score = roc_auc_score(labels_flat, predictions_flat)\\nprint(f\"AUC on test data: {auc_score:.4f}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 228
        }
      ],
      "source": [
        "# Make predictions\n",
        "\"\"\"\n",
        "predictions = model.predict(test_dataset)\n",
        "labels = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "# Flatten arrays\n",
        "predictions_flat = predictions.flatten()\n",
        "labels_flat = labels.flatten()\n",
        "\n",
        "# Compute AUC\n",
        "auc_score = roc_auc_score(labels_flat, predictions_flat)\n",
        "print(f\"AUC on test data: {auc_score:.4f}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_test_data(df_history, df_behaviors, df_articles, article_to_index, embedding_matrix, max_history_length, popularity_window_hours, top_n):\n",
        "    # Compute temporal features\n",
        "    df_behaviors = compute_session_time_features(df_behaviors)\n",
        "    df_behaviors = compute_user_activity_features(df_behaviors)\n",
        "    df_behaviors = compute_article_age(df_behaviors, df_articles)\n",
        "\n",
        "    # Compute and add in-session history\n",
        "    in_session_histories = build_in_session_histories(df_behaviors)\n",
        "    df_behaviors['in_session_history'] = df_behaviors.index.map(in_session_histories)\n",
        "\n",
        "    # Compute popularity features\n",
        "    df_behaviors = compute_popularity_features(df_behaviors, popularity_window_hours, top_n)\n",
        "\n",
        "    # Prepare user histories\n",
        "    user_histories_tensor, user_id_to_index = process_user_history(\n",
        "        df_history, article_to_index, embedding_matrix, max_history_length\n",
        "    )\n",
        "\n",
        "    # Prepare test sessions without labels\n",
        "    df_sessions = prepare_test_sessions(\n",
        "        df_behaviors, article_to_index, embedding_matrix,\n",
        "        max_articles_in_view=10, max_in_session_history=5, max_popularity_articles=10\n",
        "    )\n",
        "\n",
        "    # Create dataset without labels\n",
        "    dataset = create_tf_dataset_for_prediction(df_sessions, user_id_to_index, batch_size=32)\n",
        "\n",
        "    return dataset, user_histories_tensor, user_id_to_index, df_sessions\n",
        "\n",
        "test_dataset, test_user_histories_tensor, user_id_to_index, df_test_sessions = prepare_test_data(df_test_history, df_test_behaviors, df_test_articles, article_to_index, embedding_matrix, max_history_length, popularity_window_hours, top_N_popular_articles)"
      ],
      "metadata": {
        "id": "2kWf_C0b59KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch the user histories tensor to those from test data (not sure if we need to do that)\n",
        "#model.user_histories_tensor = test_user_histories_tensor\n",
        "\n",
        "# Extract user indices\n",
        "user_indices = df_test_sessions['user_id'].map(user_id_to_index).astype(int).to_numpy()\n",
        "\n",
        "# Extract embeddings\n",
        "article_embeddings = np.stack(df_test_sessions['article_embeddings'].to_numpy())\n",
        "in_session_embeddings = np.stack(df_test_sessions['in_session_embeddings'].to_numpy())\n",
        "popularity_embeddings = np.stack(df_test_sessions['popularity_embeddings'].to_numpy())\n",
        "\n",
        "# Create an inputs list\n",
        "inputs = [user_indices, article_embeddings, in_session_embeddings, popularity_embeddings]\n",
        "\n",
        "# Make test predictions\n",
        "test_predictions = model.predict(inputs)"
      ],
      "metadata": {
        "id": "VkQzCN7e-yg7",
        "outputId": "9e996fa9-b557-4527-9012-f99544af79f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m793/793\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ethical aspects"
      ],
      "metadata": {
        "id": "9Wzo7PA_La84"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwSSEtFsJAzV",
        "outputId": "0eafda0a-3bf6-4d2c-b39f-62cdd9651689",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of common articles: 2738\n",
            "Number of missing articles: 0\n"
          ]
        }
      ],
      "source": [
        "# Extract the column article_ids_inview from df_test_behaviors\n",
        "candidate_articles_test = df_test_behaviors['article_ids_inview'].tolist()\n",
        "\n",
        "# Convert the IDs to strings (only once necessary)\n",
        "candidate_articles_test = [[str(article) for article in articles] for articles in candidate_articles_test]\n",
        "\n",
        "# Convert all article_id values to strings in the embedding dataframe (if necessary)\n",
        "embedding_df['article_id'] = embedding_df['article_id'].astype(str)\n",
        "\n",
        "# Flatten the list of lists to create a set of unique candidate article IDs\n",
        "candidate_articles_flat = set(article for sublist in candidate_articles_test for article in sublist)\n",
        "embedding_articles = set(embedding_df['article_id'])\n",
        "\n",
        "# Common articles\n",
        "common_articles = candidate_articles_flat.intersection(embedding_articles)\n",
        "print(f\"Number of common articles: {len(common_articles)}\")\n",
        "\n",
        "# Missing articles\n",
        "missing_articles = candidate_articles_flat.difference(embedding_articles)\n",
        "print(f\"Number of missing articles: {len(missing_articles)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the Top-N recommendations (e.g., Top-5)\n",
        "top_n = 10\n",
        "recommendations_test = [np.argsort(-pred)[:top_n] for pred in predictions]\n",
        "\n",
        "# Convert indices from recommendations_test to article IDs\n",
        "article_id_recommendations_test = [\n",
        "    [candidate_articles_test[user_idx][idx] for idx in rec_list if idx < len(candidate_articles_test[user_idx])]\n",
        "    for user_idx, rec_list in enumerate(recommendations_test)\n",
        "]\n",
        "\n",
        "# Convert article_id from strings to integers\n",
        "article_id_recommendations_test = [\n",
        "    [int(article) for article in rec_list]\n",
        "    for rec_list in article_id_recommendations_test\n",
        "]\n",
        "\n",
        "# Filter: Keep only article_id values that have embeddings\n",
        "article_id_recommendations_test = [\n",
        "    [article for article in rec_list if article in article_to_index]\n",
        "    for rec_list in article_id_recommendations_test\n",
        "]\n",
        "\n",
        "# Convert article_id values to embedding indices\n",
        "mapped_recommendations = [\n",
        "    [article_to_index[article] for article in rec_list if article in article_to_index]\n",
        "    for rec_list in article_id_recommendations_test\n",
        "]\n",
        "\n",
        "# Retrieve embeddings for the recommended lists\n",
        "recommended_embeddings = [\n",
        "    [embedding_matrix[idx] for idx in rec_list]\n",
        "    for rec_list in mapped_recommendations\n",
        "]\n"
      ],
      "metadata": {
        "id": "83-RQ1xKcfer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1qbvAyuJAzV",
        "outputId": "db01636f-f082-48b4-aedc-44dc897c2bf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diversity Score: 0.1857\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def calculate_diversity_from_embeddings(recommended_embeddings):\n",
        "    \"\"\"\n",
        "    Calculates the average Diversity Score for the recommended embedding lists.\n",
        "\n",
        "    Args:\n",
        "        recommended_embeddings (list of list): Embeddings for the recommendation lists of each user.\n",
        "\n",
        "    Returns:\n",
        "        float: Average Diversity Score for all lists.\n",
        "    \"\"\"\n",
        "    diversities = []\n",
        "    for rec_list in recommended_embeddings:\n",
        "        if len(rec_list) > 1:  # If the list contains at least 2 articles\n",
        "            similarity_matrix = cosine_similarity(rec_list)\n",
        "            # Diversity: 1 - average similarity\n",
        "            pairwise_diversity = 1 - similarity_matrix[np.triu_indices(len(rec_list), k=1)].mean()\n",
        "            diversities.append(pairwise_diversity)\n",
        "        else:\n",
        "            diversities.append(0)  # If there's only 1 article, diversity = 0\n",
        "\n",
        "    # Average diversity score\n",
        "    return diversities\n",
        "diversities = calculate_diversity_from_embeddings(recommended_embeddings)\n",
        "# Call the function with the recommended embeddings\n",
        "diversity_score = np.mean(calculate_diversity_from_embeddings(recommended_embeddings))\n",
        "print(f\"Diversity Score: {diversity_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6DbH1U9JAzV"
      },
      "source": [
        "Diversity measures how different the articles in each user's ecommendation list are from one another.\n",
        "\n",
        "A Diversity Score of 0.1851 indicates that the articles within each recommendation list are not very diverse. This means that the system tends to recommend articles that are similar in content or topic for each user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "JbfNyfb8JAzW",
        "outputId": "34f41554-603f-4c0e-a929-1a4fe5b8c15b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_diversity_boxplot(diversities):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.boxplot(diversities, vert=False, patch_artist=True, boxprops=dict(facecolor='lightblue', color='blue'),\n",
        "                whiskerprops=dict(color='blue', linewidth=1.5), medianprops=dict(color='red'))\n",
        "    plt.title('Box Plot of Diversity Scores', fontsize=16)\n",
        "    plt.xlabel('Diversity Score', fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "# diversities = [0.1715, 0.432, 0.245, 0.876, 0.543, 0.332]  # Replace with your actual diversity scores\n",
        "plot_diversity_boxplot(diversities)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Diversity into loss"
      ],
      "metadata": {
        "id": "LJpXXLxpodK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines and trains the model to predict binary labels, incorporating a new objective: the diversity of predictions, alongside accuracy. It uses a custom loss function that combines the usual loss for binary classification (BinaryCrossentropy) with an additional loss function for measuring the diversity of the model's outputs. The diversity is calculated based on the predictions (embeddings) generated by the model during each training epoch. Here are the key steps in this process:\n",
        "\n",
        "Diversity Calculation:\n",
        "\n",
        "During training, the model generates predictions (embeddings) for the data.\n",
        "The diversity_loss function calculates the similarity of these predictions through a self dot product, creating a similarity matrix.\n",
        "From this matrix, calculating the average similarity (excluding the diagonal representing the similarity of an item with itself) provides the basis for calculating diversity, which is 1 minus the average similarity.\n",
        "Incorporation into the Loss Function:\n",
        "\n",
        "The final loss function combines the standard classification loss (e.g., BinaryCrossentropy) with diversity. This is achieved by adding the diversity value, scaled by a weighting factor (lambda_diversity), to the base loss.\n",
        "Adding the diversity term treats high similarity values as undesirable, encouraging the model to produce more distinct and diverse predictions.\n",
        "Training:\n",
        "\n",
        "As the model is trained, TensorFlow automatically adjusts the model's weights to minimize this combined loss, balancing the need for high accuracy and high diversity in predictions.\n",
        "Thus, diversity is not just a metric for evaluation but becomes a central part of how the model learns and adapts, seeking a good compromise between accuracy and diversity in its predictions."
      ],
      "metadata": {
        "id": "4Y1tgE-gz0dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "\n",
        "def diversity_loss(embeddings, lambda_diversity=0.1):\n",
        "    similarity_matrix = tf.matmul(embeddings, embeddings, transpose_b=True)\n",
        "    upper_triangle = tf.linalg.band_part(similarity_matrix, 0, -1)\n",
        "    num_elements = tf.cast(tf.shape(upper_triangle)[0], tf.float32)  # Convert to float32\n",
        "    average_similarity = tf.reduce_mean(upper_triangle) - tf.linalg.trace(upper_triangle) / num_elements\n",
        "    diversity = 1 - average_similarity\n",
        "    return lambda_diversity * diversity\n",
        "\n",
        "\n",
        "class CustomLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, lambda_diversity=0.1, name=\"custom_loss\"):\n",
        "        super(CustomLoss, self).__init__(name=name)\n",
        "        self.lambda_diversity = lambda_diversity\n",
        "        self.base_loss = BinaryCrossentropy()\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        # Base loss\n",
        "        base_loss_value = self.base_loss(y_true, y_pred)\n",
        "        # Add diversity loss\n",
        "        diversity_value = diversity_loss(y_pred, self.lambda_diversity)\n",
        "        return base_loss_value + diversity_value\n",
        "\n",
        "# In compile of model, use CustomLoss\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=CustomLoss(lambda_diversity=0.1),\n",
        "              metrics=[tf.keras.metrics.AUC(name=\"auc\")])\n"
      ],
      "metadata": {
        "id": "vytmNQxfoh9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=epochs_num,\n",
        "    callbacks=[reduce_lr]\n",
        ")\n"
      ],
      "metadata": {
        "id": "gIaSZE5vospM",
        "outputId": "b64da2e6-5c56-4614-d6b4-fa5b48e17ae1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 55ms/step - auc: 0.7814 - loss: 0.3554 - val_auc: 0.7117 - val_loss: 0.3795 - learning_rate: 8.7070e-04\n",
            "Epoch 2/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.7846 - loss: 0.3546 - val_auc: 0.7216 - val_loss: 0.3750 - learning_rate: 8.6364e-04\n",
            "Epoch 3/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 41ms/step - auc: 0.7863 - loss: 0.3541 - val_auc: 0.7228 - val_loss: 0.3752 - learning_rate: 8.5664e-04\n",
            "Epoch 4/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.7877 - loss: 0.3536 - val_auc: 0.7230 - val_loss: 0.3762 - learning_rate: 8.4969e-04\n",
            "Epoch 5/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.7907 - loss: 0.3529 - val_auc: 0.7206 - val_loss: 0.3768 - learning_rate: 8.4279e-04\n",
            "Epoch 6/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.7935 - loss: 0.3520 - val_auc: 0.7202 - val_loss: 0.3778 - learning_rate: 8.3596e-04\n",
            "Epoch 7/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.7964 - loss: 0.3512 - val_auc: 0.7205 - val_loss: 0.3767 - learning_rate: 8.2918e-04\n",
            "Epoch 8/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.7989 - loss: 0.3504 - val_auc: 0.7174 - val_loss: 0.3790 - learning_rate: 8.2245e-04\n",
            "Epoch 9/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.8020 - loss: 0.3495 - val_auc: 0.7153 - val_loss: 0.3798 - learning_rate: 8.1578e-04\n",
            "Epoch 10/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.8051 - loss: 0.3486 - val_auc: 0.7152 - val_loss: 0.3774 - learning_rate: 8.0916e-04\n",
            "Epoch 11/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.8075 - loss: 0.3479 - val_auc: 0.7132 - val_loss: 0.3805 - learning_rate: 8.0260e-04\n",
            "Epoch 12/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.8099 - loss: 0.3471 - val_auc: 0.7105 - val_loss: 0.3823 - learning_rate: 7.9609e-04\n",
            "Epoch 13/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.8122 - loss: 0.3464 - val_auc: 0.7111 - val_loss: 0.3831 - learning_rate: 7.8963e-04\n",
            "Epoch 14/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.8138 - loss: 0.3459 - val_auc: 0.7110 - val_loss: 0.3835 - learning_rate: 7.8323e-04\n",
            "Epoch 15/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.8151 - loss: 0.3454 - val_auc: 0.7101 - val_loss: 0.3849 - learning_rate: 7.7688e-04\n",
            "Epoch 16/16\n",
            "\u001b[1m773/773\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - auc: 0.8182 - loss: 0.3445 - val_auc: 0.7080 - val_loss: 0.3857 - learning_rate: 7.7057e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78230502fe20>"
            ]
          },
          "metadata": {},
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Serendipity"
      ],
      "metadata": {
        "id": "KC-r5DaTolY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_dataset)\n",
        "labels = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "# Flatten arrays\n",
        "predictions_flat = predictions.flatten()\n",
        "labels_flat = labels.flatten()\n",
        "\n",
        "# Compute AUC\n",
        "auc_score = roc_auc_score(labels_flat, predictions_flat)\n",
        "print(f\"AUC on test data: {auc_score:.4f}\")"
      ],
      "metadata": {
        "id": "EFiOXQdjQq5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6YNJCNrJAzW",
        "outputId": "da0d4e2d-69aa-4b19-c66a-ceab17277aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'article_ids_clicked'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'article_ids_clicked'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-238-3117440bde88>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclicked_articles_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test_behaviors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article_ids_clicked'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Ensure that IDs are strings (to match other data formats)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'article_ids_clicked'"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "clicked_articles_test = df_test_behaviors['article_ids_clicked'].tolist()\n",
        "\n",
        "# Ensure that IDs are strings (to match other data formats)\n",
        "clicked_articles_test = [[str(article) for article in articles] for articles in clicked_articles_test]\n",
        "\n",
        "# Count the frequency of articles in the training set\n",
        "article_counts = Counter(df_train_behaviors['article_id'])\n",
        "# Extract the Top-K most popular articles (e.g., Top-10)\n",
        "baseline_articles = [article for article, _ in article_counts.most_common(10)]\n",
        "\n",
        "# Ensure that IDs are strings\n",
        "baseline_articles = [str(article) for article in baseline_articles]\n",
        "\n",
        "# Convert clicked articles to integers\n",
        "clicked_articles_test = [[int(article) for article in articles] for articles in clicked_articles_test]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_serendipity(recommendations, clicked_articles, baseline_articles):\n",
        "    \"\"\"\n",
        "    Calculate the Serendipity Score for recommendations.\n",
        "\n",
        "    Args:\n",
        "        recommendations (list of list): Lists of recommended article IDs for each user.\n",
        "        clicked_articles (list of list): Lists of article IDs clicked by each user.\n",
        "        baseline_articles (list): List of popular article IDs used as a baseline.\n",
        "\n",
        "    Returns:\n",
        "        float: Average Serendipity Score across all users.\n",
        "    \"\"\"\n",
        "    serendipities = []\n",
        "    for rec_list, clicked in zip(recommendations, clicked_articles):\n",
        "        # Unexpected recommendations: not in baseline\n",
        "        unexpected = [article for article in rec_list if article not in baseline_articles]\n",
        "\n",
        "        # Relevant unexpected recommendations: clicked by the user\n",
        "        relevant = [article for article in unexpected if article in clicked]\n",
        "\n",
        "        # Serendipity score for the user\n",
        "        serendipity_score = len(relevant) / len(rec_list) if len(rec_list) > 0 else 0\n",
        "        serendipities.append(serendipity_score)\n",
        "\n",
        "    return serendipities\n",
        "serendipities = calculate_serendipity(article_id_recommendations_test, clicked_articles_test, baseline_articles)\n",
        "serendipity_score = np.mean(serendipities)\n",
        "print(f\"Serendipity Score: {serendipity_score:.4f}\")"
      ],
      "metadata": {
        "id": "f_K7ghz7cqq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slyPv6SwJAzX"
      },
      "source": [
        "Serendipity measures the extent to which a recommendation system provides unexpected yet relevant recommendations to the user. It goes beyond accuracy by evaluating whether the recommendations surprise the user in a positive way.\n",
        "\n",
        "A Serendipity Score of 0.1173 indicates that the system provides a small percentage of unexpected but relevant recommendations to the users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDeW1d6_JAzX"
      },
      "outputs": [],
      "source": [
        "def plot_serendipity_histogram(serendipities):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(serendipities, bins=20, color='green', alpha=0.7, edgecolor='black')\n",
        "    plt.title('Distribution of Serendipity Scores', fontsize=16)\n",
        "    plt.xlabel('Serendipity Score', fontsize=14)\n",
        "    plt.ylabel('Frequency', fontsize=14)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "plot_serendipity_histogram(serendipities)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Novelty"
      ],
      "metadata": {
        "id": "XE5f8zMF1LMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Total number of articles in the training set\n",
        "total_count = sum(article_counts.values())\n",
        "\n",
        "# Create article_popularity dictionary\n",
        "article_popularity = {article: count / total_count for article, count in article_counts.items()}"
      ],
      "metadata": {
        "id": "Dk5t4x06cuaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhG_J5KGJAzX"
      },
      "outputs": [],
      "source": [
        "def calculate_novelty(recommendations, article_popularity):\n",
        "    \"\"\"\n",
        "    Calculate the Novelty Score for recommendations.\n",
        "\n",
        "    Args:\n",
        "        recommendations (list of list): Lists of recommended article IDs for each user.\n",
        "        article_popularity (dict): Dictionary mapping article IDs to popularity probabilities.\n",
        "\n",
        "    Returns:\n",
        "        float: Average Novelty Score across all users.\n",
        "    \"\"\"\n",
        "    novelty_scores = []\n",
        "    for rec_list in recommendations:\n",
        "        novelty_sum = 0\n",
        "        for article in rec_list:\n",
        "            # Get the popularity of the article, default to a very small probability\n",
        "            prob = article_popularity.get(article, 1e-6)\n",
        "            novelty_sum += -np.log(prob)\n",
        "\n",
        "        # Average novelty score for the user's recommendation list\n",
        "        novelty_scores.append(novelty_sum / len(rec_list) if len(rec_list) > 0 else 0)\n",
        "\n",
        "    return novelty_scores\n",
        "novelty_scores = calculate_novelty(article_id_recommendations_test, article_popularity)\n",
        "novelty_score = np.mean(novelty_scores)\n",
        "print(f\"Novelty Score: {novelty_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8UHBUDlJAzX"
      },
      "source": [
        "Novelty measures how \"new\" or \"unfamiliar\" the recommended items are to the user, often based on their global popularity:\n",
        "\n",
        "Highly popular items are considered less novel because they are likely to be known by most users.\n",
        "Less popular items are considered more novel because they have a higher likelihood of being new to the user.\n",
        "\n",
        "A Novelty Score of 13.8155 indicates that the recommended articles are highly novel, meaning the system suggests articles that are generally less popular and not commonly seen by users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufEAmPirJAzX"
      },
      "outputs": [],
      "source": [
        "def plot_novelty_histogram(novelty_scores):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(novelty_scores, bins=20, color='orange', alpha=0.7, edgecolor='black')\n",
        "    plt.title('Distribution of Novelty Scores', fontsize=16)\n",
        "    plt.xlabel('Novelty Score', fontsize=14)\n",
        "    plt.ylabel('Frequency', fontsize=14)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "plot_novelty_histogram(novelty_scores)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_articles = embedding_matrix.shape[0]\n"
      ],
      "metadata": {
        "id": "50waIBfTczIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZXSoPqeJAzX"
      },
      "outputs": [],
      "source": [
        "def calculate_item_coverage(recommendations, total_articles):\n",
        "    \"\"\"\n",
        "    Calculate the Item Coverage for recommendations.\n",
        "\n",
        "    Args:\n",
        "        recommendations (list of list): Lists of recommended article IDs for each user.\n",
        "        total_articles (int): Total number of articles available in the catalog.\n",
        "\n",
        "    Returns:\n",
        "        float: Coverage Score as a fraction of total articles covered.\n",
        "    \"\"\"\n",
        "    # Unique articles recommended across all users\n",
        "    recommended_articles = set(article for rec_list in recommendations for article in rec_list)\n",
        "\n",
        "    # Coverage is the fraction of unique articles recommended\n",
        "    return recommended_articles, len(recommended_articles) / total_articles\n",
        "recommended_articles,_ = calculate_item_coverage(article_id_recommendations_test, total_articles)\n",
        "_,coverage_score = calculate_item_coverage(article_id_recommendations_test, total_articles)\n",
        "print(f\"Item Coverage Score: {coverage_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzCizCSSJAzX"
      },
      "source": [
        "Coverage evaluates how well the recommendation system utilizes the available content:\n",
        "\n",
        "High Coverage: Suggests that the system explores a wide variety of articles.\n",
        "Low Coverage: Indicates that the system focuses only on a limited set of articles, potentially overlooking many others.\n",
        "\n",
        "An Item Coverage Score of 0.0316 means that the recommendation system uses only about 3.16% of the total articles in the catalog across all its recommendations.\n",
        "This indicates a strong bias toward a small subset of articles, likely the most relevant or popular ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opkPe7AGJAzX"
      },
      "outputs": [],
      "source": [
        "def plot_coverage_pie(recommended_articles, total_articles):\n",
        "    coverage_score = len(recommended_articles) / total_articles\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.pie(\n",
        "        [coverage_score, 1 - coverage_score],\n",
        "        labels=[\"Covered\", \"Not Covered\"],\n",
        "        autopct=\"%1.1f%%\",\n",
        "        colors=[\"lightblue\", \"lightgrey\"],\n",
        "        startangle=140\n",
        "    )\n",
        "    plt.title(\"Proportion of Catalog Covered\", fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "plot_coverage_pie(recommended_articles, total_articles)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}