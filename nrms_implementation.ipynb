{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW779fn0Okag",
        "outputId": "e3de3e13-d692-4c8c-e5e1-c7174a51eaa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install necessary libraries\n",
        "!pip install tensorflow pandas numpy scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import required libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "KGdGdZ1MOv5n"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Mount Google Drive if using datasets stored there\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOxtE_LBPCv8",
        "outputId": "79fa6749-be71-4e09-f72e-1a4638b955be"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Set file paths (modify these paths based on where your files are stored)\n",
        "train_path = '/content/drive/MyDrive/deep_learning_project/data/train'  # Adjust to the correct path\n",
        "test_path = '/content/drive/MyDrive/deep_learning_project/data/validation'  # Adjust to the correct path"
      ],
      "metadata": {
        "id": "4Dx4yPBjPCs0"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Load the train and test datasets\n",
        "df_train_history = pd.read_parquet(os.path.join(train_path, 'history.parquet'))\n",
        "df_train_behaviors = pd.read_parquet(os.path.join(train_path, 'behaviors.parquet'))\n",
        "df_train_articles = pd.read_parquet(os.path.join(train_path, 'articles.parquet'))\n",
        "\n",
        "df_test_history = pd.read_parquet(os.path.join(test_path, 'history.parquet'))\n",
        "df_test_behaviors = pd.read_parquet(os.path.join(test_path, 'behaviors.parquet'))\n",
        "df_test_articles = pd.read_parquet(os.path.join(test_path, 'articles.parquet'))"
      ],
      "metadata": {
        "id": "D5TBHs8OPCqM"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Ensure consistent data types for merging in both train and test datasets\n",
        "df_train_behaviors['article_id'] = df_train_behaviors['article_id'].fillna(-1).astype(int).astype(str)\n",
        "df_train_articles['article_id'] = df_train_articles['article_id'].astype(str)\n",
        "df_test_behaviors['article_id'] = df_test_behaviors['article_id'].fillna(-1).astype(int).astype(str)\n",
        "df_test_articles['article_id'] = df_test_articles['article_id'].astype(str)"
      ],
      "metadata": {
        "id": "vZ1TehMbPCnU"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Merge behaviors with articles for both train and test datasets\n",
        "df_train = pd.merge(df_train_behaviors, df_train_articles, left_on='article_id', right_on='article_id', how='left')\n",
        "df_test = pd.merge(df_test_behaviors, df_test_articles, left_on='article_id', right_on='article_id', how='left')"
      ],
      "metadata": {
        "id": "mS26TxxcPCkn"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsQ5Ml47PErg",
        "outputId": "8d830b1b-bfac-4e59-b640-ffae724cf576"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['impression_id', 'article_id', 'impression_time', 'read_time',\n",
            "       'scroll_percentage', 'device_type', 'article_ids_inview',\n",
            "       'article_ids_clicked', 'user_id', 'is_sso_user', 'gender', 'postcode',\n",
            "       'age', 'is_subscriber', 'session_id', 'next_read_time',\n",
            "       'next_scroll_percentage', 'title', 'subtitle', 'last_modified_time',\n",
            "       'premium', 'body', 'published_time', 'image_ids', 'article_type', 'url',\n",
            "       'ner_clusters', 'entity_groups', 'topics', 'category', 'subcategory',\n",
            "       'category_str', 'total_inviews', 'total_pageviews', 'total_read_time',\n",
            "       'sentiment_score', 'sentiment_label'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_test.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWyw2HA8WasA",
        "outputId": "67b729aa-a082-43a2-a963-0cf65c279605"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['impression_id', 'article_id', 'impression_time', 'read_time',\n",
            "       'scroll_percentage', 'device_type', 'article_ids_inview',\n",
            "       'article_ids_clicked', 'user_id', 'is_sso_user', 'gender', 'postcode',\n",
            "       'age', 'is_subscriber', 'session_id', 'next_read_time',\n",
            "       'next_scroll_percentage', 'title', 'subtitle', 'last_modified_time',\n",
            "       'premium', 'body', 'published_time', 'image_ids', 'article_type', 'url',\n",
            "       'ner_clusters', 'entity_groups', 'topics', 'category', 'subcategory',\n",
            "       'category_str', 'total_inviews', 'total_pageviews', 'total_read_time',\n",
            "       'sentiment_score', 'sentiment_label'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Define the process_data function to extract input data for NRMS model\n",
        "\n",
        "def process_data(data, max_history_len=10, max_candidate_len=5):\n",
        "    \"\"\"\n",
        "    Process dataset to extract user history, candidate articles, and labels for model training.\n",
        "\n",
        "    Args:\n",
        "        data (DataFrame): The dataset containing merged user, behavior, and article information.\n",
        "        max_history_len (int): Maximum number of articles in user history.\n",
        "        max_candidate_len (int): Maximum number of candidate articles shown in an impression.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Arrays of user history, candidate articles, and labels.\n",
        "    \"\"\"\n",
        "    user_history_col = 'article_ids_clicked'  # Kolumna z historią artykułów klikniętych\n",
        "    candidate_articles_col = 'article_ids_inview'  # Kolumna z artykułami-kandydatami\n",
        "\n",
        "    # Mapowanie `article_id` na indeksy w embedding_matrix\n",
        "    data[user_history_col] = data[user_history_col].apply(\n",
        "        lambda x: [article_to_index[article] for article in x[:max_history_len] if article in article_to_index] + [0] * (max_history_len - len(x))\n",
        "    )\n",
        "    data[candidate_articles_col] = data[candidate_articles_col].apply(\n",
        "        lambda x: [article_to_index[article] for article in x[:max_candidate_len] if article in article_to_index] + [0] * (max_candidate_len - len(x))\n",
        "    )\n",
        "\n",
        "    # Przygotowanie `user_history` i `candidate_articles` w wymaganych kształtach\n",
        "    user_history = np.array(data[user_history_col].tolist(), dtype=np.int32)  # Kształt (None, 10)\n",
        "    candidate_articles = np.array(data[candidate_articles_col].tolist(), dtype=np.int32)  # Kształt (None, 5)\n",
        "\n",
        "    # Generowanie etykiet na podstawie tego, czy artykuł był kliknięty\n",
        "    labels = [\n",
        "        [(1 if candidate in clicked else 0) for candidate in candidates[:max_candidate_len]]\n",
        "        for clicked, candidates in zip(data[user_history_col], data[candidate_articles_col])\n",
        "    ]\n",
        "    labels = np.array(labels, dtype=np.int32)\n",
        "\n",
        "    return user_history, candidate_articles, labels\n"
      ],
      "metadata": {
        "id": "X45ab4lCPCiJ"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Process train and test data\n",
        "X_train_user_history, X_train_candidates, y_train = process_data(df_train)\n",
        "X_test_user_history, X_test_candidates, y_test = process_data(df_test)"
      ],
      "metadata": {
        "id": "f-rJtgqOPCfd"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Import the embedding fle provided by the competition organizers\n",
        "embedding_df = pd.read_parquet('/content/drive/MyDrive/deep_learning_project/data/embeddings/document_vector.parquet')\n",
        "\n",
        "# Check the embedding vectors dimension\n",
        "embedding_dim = len(embedding_df['document_vector'].iloc[0])\n",
        "\n",
        "# Mapping article_id -> embedding index\n",
        "article_to_index = {article_id: idx for idx, article_id in enumerate(embedding_df['article_id'])}\n",
        "\n",
        "# Initialisation of embedding matrix\n",
        "num_articles = len(article_to_index)\n",
        "embedding_matrix = np.zeros((num_articles, embedding_dim))\n",
        "\n",
        "# Puopulate the embedding matrix\n",
        "for idx, row in embedding_df.iterrows():\n",
        "    article_id = row['article_id']\n",
        "    vector = np.array(row['document_vector'])\n",
        "    if article_id in article_to_index:\n",
        "        index = article_to_index[article_id]\n",
        "        embedding_matrix[index] = vector"
      ],
      "metadata": {
        "id": "sHDtILv8PCaI"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Updated NRMS Model with Improved Architecture and Metadata\n",
        "\n",
        "class ImprovedNRMSModel:\n",
        "    \"\"\"Improved NRMS model with deeper architecture, metadata features, and tunable embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, word_embedding_dim=embedding_dim, head_num=16, head_dim=25, attention_hidden_dim=80, dropout_rate=0.35):\n",
        "        self.word_embedding_dim = word_embedding_dim\n",
        "        self.head_num = head_num\n",
        "        self.head_dim = head_dim\n",
        "        self.attention_hidden_dim = attention_hidden_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "    def build_news_encoder(self):\n",
        "        \"\"\"Build the news encoder with article embeddings.\"\"\"\n",
        "        input_layer = layers.Input(shape=(), dtype=\"int32\")  # Pojedynczy indeks artykułu\n",
        "\n",
        "        # Embedding layer with custom embedding matrix\n",
        "        embedding_layer = layers.Embedding(input_dim=embedding_matrix.shape[0],\n",
        "                                          output_dim=embedding_matrix.shape[1],\n",
        "                                          embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "                                          trainable=True)(input_layer)\n",
        "\n",
        "        # Dodaj inne warstwy, jeśli potrzebne (np. Dropout, LayerNormalization)\n",
        "        output = layers.Dense(self.word_embedding_dim, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "        model = Model(inputs=input_layer, outputs=output, name=\"news_encoder\")\n",
        "        return model\n",
        "\n",
        "    def build_user_encoder(self, news_encoder):\n",
        "        \"\"\"Build user encoder to capture user interests.\"\"\"\n",
        "        input_layer = layers.Input(shape=(10,), dtype=\"int32\")  # Sekwencja 10 artykułów dla użytkownika\n",
        "\n",
        "        # Przetwarzanie sekwencji za pomocą `news_encoder` bez `TimeDistributed`\n",
        "        clicked_news_embeddings = layers.Lambda(lambda x: tf.stack([news_encoder(article) for article in tf.unstack(x, axis=1)], axis=1))(input_layer)\n",
        "\n",
        "        # Użycie Self-Attention\n",
        "        y = layers.MultiHeadAttention(num_heads=self.head_num, key_dim=self.head_dim)(clicked_news_embeddings, clicked_news_embeddings)\n",
        "        y = layers.Dropout(self.dropout_rate)(y)\n",
        "        y = layers.LayerNormalization()(y)\n",
        "\n",
        "        # Dodanie warstwy uwagi\n",
        "        attention_output = layers.Attention()([y, y])\n",
        "        output = layers.GlobalAveragePooling1D()(attention_output)\n",
        "\n",
        "        model = Model(inputs=input_layer, outputs=output, name=\"user_encoder\")\n",
        "        return model\n",
        "\n",
        "    def build_model(self, metadata_dim=None):\n",
        "        \"\"\"Build the full NRMS model with metadata concatenation.\"\"\"\n",
        "        user_history = layers.Input(shape=(10,), dtype=\"int32\", name=\"user_history\")  # Użytkownik - 10 klikniętych artykułów\n",
        "        candidate_news = layers.Input(shape=(5,), dtype=\"int32\", name=\"candidate_news\")\n",
        "\n",
        "        news_encoder = self.build_news_encoder()\n",
        "        user_encoder = self.build_user_encoder(news_encoder)\n",
        "\n",
        "        # Encode user history and candidate articles\n",
        "        user_repr = user_encoder(user_history)\n",
        "        candidate_repr = layers.Lambda(lambda x: tf.stack([news_encoder(article) for article in tf.unstack(x, axis=1)], axis=1))(candidate_news)\n",
        "\n",
        "        # Dot product for similarity scores and softmax for probabilities\n",
        "        scores = layers.Dot(axes=-1)([candidate_repr, user_repr])\n",
        "        probs = layers.Activation(\"softmax\")(scores)\n",
        "\n",
        "        # Compile the model with custom metrics or additional regularization if needed\n",
        "        model = Model(inputs=[user_history, candidate_news], outputs=probs)\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=\"categorical_crossentropy\", metrics=[\"AUC\"])\n",
        "\n",
        "        return model"
      ],
      "metadata": {
        "id": "FGdvjGRZPCXI"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Instantiate and compile the Improved NRMS model\n",
        "improved_model = ImprovedNRMSModel()\n",
        "model = improved_model.build_model()"
      ],
      "metadata": {
        "id": "acaN-Wk9PCT2"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 13: Define callbacks for training (Learning Rate Scheduler)\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=2, min_lr=1e-6)\n"
      ],
      "metadata": {
        "id": "NasvzdvvPCPX"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 14: Train the model with updated parameters\n",
        "history = model.fit([X_train_user_history, X_train_candidates], y_train,\n",
        "                    validation_data=([X_test_user_history, X_test_candidates], y_test),\n",
        "                    epochs=20, batch_size=64, callbacks=[lr_scheduler])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suNW9T-8PCGn",
        "outputId": "d394f829-2fa1-4e9d-858a-559a7b8060d9"
      },
      "execution_count": 93,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 7ms/step - AUC: 0.5390 - loss: 0.9894 - val_AUC: 0.7744 - val_loss: 0.7222 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m  59/3639\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - AUC: 0.7938 - loss: 0.7650"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/callback_list.py:96: UserWarning: Learning rate reduction is conditioned on metric `val_auc` which is not available. Available metrics are: AUC,loss,val_AUC,val_loss,learning_rate.\n",
            "  callback.on_epoch_end(epoch, logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - AUC: 0.8144 - loss: 0.7028 - val_AUC: 0.8454 - val_loss: 0.5928 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - AUC: 0.8724 - loss: 0.5898 - val_AUC: 0.8631 - val_loss: 0.5419 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - AUC: 0.8882 - loss: 0.5368 - val_AUC: 0.8605 - val_loss: 0.5371 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - AUC: 0.8944 - loss: 0.5022 - val_AUC: 0.8581 - val_loss: 0.5373 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - AUC: 0.8977 - loss: 0.4792 - val_AUC: 0.8646 - val_loss: 0.5184 - learning_rate: 1.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - AUC: 0.8978 - loss: 0.4621 - val_AUC: 0.8649 - val_loss: 0.5184 - learning_rate: 1.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - AUC: 0.8987 - loss: 0.4508 - val_AUC: 0.8617 - val_loss: 0.5320 - learning_rate: 1.0000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - AUC: 0.8994 - loss: 0.4409 - val_AUC: 0.8564 - val_loss: 0.5561 - learning_rate: 1.0000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - AUC: 0.8977 - loss: 0.4399 - val_AUC: 0.8684 - val_loss: 0.5250 - learning_rate: 1.0000e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - AUC: 0.8978 - loss: 0.4342 - val_AUC: 0.8601 - val_loss: 0.5627 - learning_rate: 1.0000e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - AUC: 0.8987 - loss: 0.4323 - val_AUC: 0.8659 - val_loss: 0.5524 - learning_rate: 1.0000e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - AUC: 0.8979 - loss: 0.4334 - val_AUC: 0.8549 - val_loss: 0.6060 - learning_rate: 1.0000e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - AUC: 0.8972 - loss: 0.4367 - val_AUC: 0.8555 - val_loss: 0.6191 - learning_rate: 1.0000e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - AUC: 0.8955 - loss: 0.4410 - val_AUC: 0.8568 - val_loss: 0.6289 - learning_rate: 1.0000e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - AUC: 0.8965 - loss: 0.4411 - val_AUC: 0.8490 - val_loss: 0.6766 - learning_rate: 1.0000e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - AUC: 0.8966 - loss: 0.4436 - val_AUC: 0.8473 - val_loss: 0.6988 - learning_rate: 1.0000e-04\n",
            "Epoch 18/20\n",
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - AUC: 0.8946 - loss: 0.4534 - val_AUC: 0.8431 - val_loss: 0.7353 - learning_rate: 1.0000e-04\n",
            "Epoch 19/20\n",
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - AUC: 0.8950 - loss: 0.4556 - val_AUC: 0.8460 - val_loss: 0.7420 - learning_rate: 1.0000e-04\n",
            "Epoch 20/20\n",
            "\u001b[1m3639/3639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - AUC: 0.8944 - loss: 0.4614 - val_AUC: 0.8528 - val_loss: 0.7226 - learning_rate: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 15: Evaluate the model on test data\n",
        "test_auc = model.evaluate([X_test_user_history, X_test_candidates], y_test, return_dict=True)['AUC']\n",
        "print(f\"AUC Score on Test Data: {test_auc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mYGDCN0R30X",
        "outputId": "19101783-41c5-42e8-970b-9f70840bce69"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7646/7646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - AUC: 0.8541 - loss: 0.7161\n",
            "AUC Score on Test Data: 0.8527636528015137\n"
          ]
        }
      ]
    }
  ]
}