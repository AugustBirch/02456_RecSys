{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlJRiNKkvF4S"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YgIaKe7SvWNH",
    "outputId": "97df8d32-7b2e-4747-cb60-3e49d920c9db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive if using datasets stored there\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OJ0hkpynvWBw"
   },
   "outputs": [],
   "source": [
    "# Load the Parquet file into a Pandas DataFrame to inspect columns\n",
    "file_paths = {\n",
    "    \"behaviors\": \"/content/drive/My Drive/deeplearningdata/behaviors.parquet\",  # Properly enclosed in quotes\n",
    "    \"history\": \"/content/drive/My Drive/deeplearningdata/history.parquet\",\n",
    "    \"articles\": \"/content/drive/My Drive/deeplearningdata/articles.parquet\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KDAL5CqbvV1M"
   },
   "outputs": [],
   "source": [
    "# Select specific features by their names\n",
    "selected_features = {\n",
    "    \"behaviors\": [\"article_id\", \"impression_id\", \"impression_time\", \"article_ids_inview\", \"user_id\", \"session_id\", \"article_ids_clicked\"],\n",
    "    \"history\": [\"article_id_fixed\", \"user_id\", \"impression_time_fixed\", \"scroll_percentage_fixed\", \"read_time_fixed\"],\n",
    "    \"articles\": [\"article_id\", \"published_time\", \"ner_clusters\", \"topics\", \"category\", \"total_read_time\", \"total_pageviews\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6v2wxny5_ulk",
    "outputId": "b1d2ba9e-31f6-493a-fb8b-4d2257f761e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: behaviors\n",
      "article_id                    float64\n",
      "impression_id                  uint32\n",
      "impression_time        datetime64[us]\n",
      "article_ids_inview             object\n",
      "user_id                        uint32\n",
      "session_id                     uint32\n",
      "article_ids_clicked            object\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "Dataset: history\n",
      "article_id_fixed           object\n",
      "user_id                    uint32\n",
      "impression_time_fixed      object\n",
      "scroll_percentage_fixed    object\n",
      "read_time_fixed            object\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "Dataset: articles\n",
      "article_id                  int32\n",
      "published_time     datetime64[us]\n",
      "ner_clusters               object\n",
      "topics                     object\n",
      "category                    int16\n",
      "total_read_time           float32\n",
      "total_pageviews           float64\n",
      "dtype: object\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Function to load a small sample and print data types\n",
    "def print_feature_dtypes(file_path, selected_columns, dataset_name):\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    data = pd.read_parquet(file_path, columns=selected_columns)\n",
    "    print(data.dtypes)\n",
    "    print(\"-\" * 40)\n",
    "# Print data types for each dataset\n",
    "for dataset_name, file_path in file_paths.items():\n",
    "    selected_columns = selected_features[dataset_name]\n",
    "    print_feature_dtypes(file_path, selected_columns, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n241YkiZAqFI",
    "outputId": "d1e53003-8290-444a-ab5e-708655d24e02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article_id': <tf.Tensor: shape=(), dtype=float32, numpy=nan>, 'impression_id': <tf.Tensor: shape=(), dtype=int64, numpy=48401>, 'impression_time': <tf.Tensor: shape=(), dtype=string, numpy=b'2023-05-21T21:06:50'>, 'article_ids_inview': <tf.Tensor: shape=(), dtype=string, numpy=b'[9774516 9771051 9770028 9775402 9774461 9759544 9773947 9142581 9775331\\n 9775371 9759966]'>, 'user_id': <tf.Tensor: shape=(), dtype=int64, numpy=22779>, 'session_id': <tf.Tensor: shape=(), dtype=int64, numpy=21>, 'article_ids_clicked': <tf.Tensor: shape=(), dtype=string, numpy=b'[9759966]'>}\n",
      "{'article_id': <tf.Tensor: shape=(), dtype=float32, numpy=9778745.0>, 'impression_id': <tf.Tensor: shape=(), dtype=int64, numpy=152513>, 'impression_time': <tf.Tensor: shape=(), dtype=string, numpy=b'2023-05-24T07:31:26'>, 'article_ids_inview': <tf.Tensor: shape=(), dtype=string, numpy=b'[9778669 9778736 9778623 9089120 9778661 9777492 9778718 9778657 9778682\\n 9482970 9718262 9718298 9778728 9080070 9420172 9717914 9777397]'>, 'user_id': <tf.Tensor: shape=(), dtype=int64, numpy=150224>, 'session_id': <tf.Tensor: shape=(), dtype=int64, numpy=298>, 'article_ids_clicked': <tf.Tensor: shape=(), dtype=string, numpy=b'[9778661]'>}\n",
      "{'article_id': <tf.Tensor: shape=(), dtype=float32, numpy=nan>, 'impression_id': <tf.Tensor: shape=(), dtype=int64, numpy=155390>, 'impression_time': <tf.Tensor: shape=(), dtype=string, numpy=b'2023-05-24T07:30:33'>, 'article_ids_inview': <tf.Tensor: shape=(), dtype=string, numpy=b'[9778369 9777856 9778500 9778021 9778627 9778351 9778155 9778226 9777034\\n 9778628 9778448]'>, 'user_id': <tf.Tensor: shape=(), dtype=int64, numpy=160892>, 'session_id': <tf.Tensor: shape=(), dtype=int64, numpy=401>, 'article_ids_clicked': <tf.Tensor: shape=(), dtype=string, numpy=b'[9777856]'>}\n",
      "{'article_id': <tf.Tensor: shape=(), dtype=float32, numpy=nan>, 'impression_id': <tf.Tensor: shape=(), dtype=int64, numpy=214679>, 'impression_time': <tf.Tensor: shape=(), dtype=string, numpy=b'2023-05-23T05:25:40'>, 'article_ids_inview': <tf.Tensor: shape=(), dtype=string, numpy=b'[9776715 9776406 9776566 9776071 9776808 9776246 9776497 9776046 9776855]'>, 'user_id': <tf.Tensor: shape=(), dtype=int64, numpy=1001055>, 'session_id': <tf.Tensor: shape=(), dtype=int64, numpy=1357>, 'article_ids_clicked': <tf.Tensor: shape=(), dtype=string, numpy=b'[9776566]'>}\n",
      "{'article_id': <tf.Tensor: shape=(), dtype=float32, numpy=nan>, 'impression_id': <tf.Tensor: shape=(), dtype=int64, numpy=214681>, 'impression_time': <tf.Tensor: shape=(), dtype=string, numpy=b'2023-05-23T05:31:54'>, 'article_ids_inview': <tf.Tensor: shape=(), dtype=string, numpy=b'[9775202 9776855 9776688 9771995 9776583 9776553 9695098 9776071 9776691\\n 9776449 9774840 9776560 9776246 9775804 9776690 9776369 9776292 9776570]'>, 'user_id': <tf.Tensor: shape=(), dtype=int64, numpy=1001055>, 'session_id': <tf.Tensor: shape=(), dtype=int64, numpy=1358>, 'article_ids_clicked': <tf.Tensor: shape=(), dtype=string, numpy=b'[9776553]'>}\n"
     ]
    }
   ],
   "source": [
    "def parquet_generator(file_path, selected_columns, chunk_size=1000):\n",
    "    \"\"\"Generator function to read specific columns from a parquet file in chunks using pyarrow.\"\"\"\n",
    "    table = pq.read_table(file_path, columns=selected_columns)\n",
    "    df = table.to_pandas()\n",
    "    for start in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[start:start + chunk_size]\n",
    "        for _, row in chunk.iterrows():\n",
    "            yield {\n",
    "                col: (\n",
    "                    str(row[col]) if isinstance(row[col], (list, pd.Series, np.ndarray)) else\n",
    "                    row[col].isoformat() if isinstance(row[col], pd.Timestamp) else\n",
    "                    row[col]\n",
    "                )\n",
    "                for col in selected_columns\n",
    "            }\n",
    "\n",
    "def infer_signature(file_path, selected_columns):\n",
    "    \"\"\"Infers output signature for TensorFlow dataset.\"\"\"\n",
    "    table = pq.read_table(file_path, columns=selected_columns)\n",
    "    schema_df = table.to_pandas().iloc[:0]  # Fetch schema without rows\n",
    "    output_signature = {\n",
    "        col: tf.TensorSpec(\n",
    "            shape=(),\n",
    "            dtype=tf.string if pd.api.types.is_object_dtype(dtype) or col in [\"article_ids_inview\", \"article_ids_clicked\", \"impression_time\"] else\n",
    "                  tf.float32 if pd.api.types.is_float_dtype(dtype) else\n",
    "                  tf.int64 if pd.api.types.is_integer_dtype(dtype) else\n",
    "                  tf.string  # Fallback for unsupported dtypes\n",
    "        )\n",
    "        for col, dtype in schema_df.dtypes.items()\n",
    "    }\n",
    "    return output_signature\n",
    "\n",
    "def create_dataset(file_path, selected_columns, chunk_size=1000):\n",
    "    \"\"\"Creates a TensorFlow dataset from a parquet file using a generator.\"\"\"\n",
    "    output_signature = infer_signature(file_path, selected_columns)\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: parquet_generator(file_path, selected_columns, chunk_size),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "# Create datasets for each parquet file\n",
    "datasets = {\n",
    "    key: create_dataset(file_paths[key], selected_features[key])\n",
    "    for key in file_paths.keys()\n",
    "}\n",
    "\n",
    "# Example usage: iterate over the first few rows of the behaviors dataset\n",
    "for row in datasets[\"behaviors\"].take(5):\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
